{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Hide TF deprecation messages\n",
    "import numpy as np\n",
    "import pickle\n",
    "import modules\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chair_meshes_paths = list(glob(\"chairs_ngon/*.obj\"))\n",
    "with open(\"chairs_split_dict.pickle\", 'rb') as f:\n",
    "    chairs_split_dict = pickle.load(f)\n",
    "train_paths = []\n",
    "val_paths = []\n",
    "test_paths = []\n",
    "for c in chair_meshes_paths:\n",
    "    try:\n",
    "        split = chairs_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if split =='train':\n",
    "        train_paths.append(c)\n",
    "    elif split =='val':\n",
    "        val_paths.append(c)\n",
    "    else:\n",
    "        test_paths.append(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "captions = pd.read_csv(\"captions_tablechair.csv\").dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('chair_to_text_full.pickle', 'rb') as f:\n",
    "    chair_to_text = pickle.load(f)\n",
    "with open('word_to_int_full.pickle', 'rb') as f:\n",
    "    word_to_int = pickle.load(f)\n",
    "with open('int_to_word_full.pickle', 'rb') as f:\n",
    "    int_to_word = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text2shape(paths):\n",
    "    for path in paths:\n",
    "        mesh_dict = data_utils.load_process_mesh(path)\n",
    "        if len(mesh_dict['vertices'])>800:\n",
    "            continue\n",
    "        if len(mesh_dict['faces'])>2800:\n",
    "            continue\n",
    "        try:\n",
    "            texts =  chair_to_text[path.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        text = texts[0][:max_length]\n",
    "        mesh_dict['text_feature'] = np.pad(text, (0,max_length-len(text)))\n",
    "        yield mesh_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Text2ShapeDataset = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(test_paths),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "            'text_feature':tf.TensorShape(max_length)})\n",
    "vertex_model_dataset = data_utils.make_vertex_model_dataset(Text2ShapeDataset, apply_random_shift=False)\n",
    "vertex_model_dataset = vertex_model_dataset.repeat()\n",
    "vertex_model_dataset = vertex_model_dataset.padded_batch(16, padded_shapes=vertex_model_dataset.output_shapes)\n",
    "vertex_model_dataset = vertex_model_dataset.prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vertex_model = modules.TextToVertexModel(\n",
    "    decoder_config=dict(\n",
    "      hidden_size=128,\n",
    "      fc_size=512,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=24,\n",
    "      dropout_rate=0.4,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True\n",
    "      ),\n",
    "    path_to_embeddings=\"glove/glove.6B.100d.txt\",\n",
    "    embedding_dims = 100,\n",
    "    quantization_bits=8,\n",
    "    vocab = word_to_int,\n",
    "    max_num_input_verts=800,  # number of vertices in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "    use_discrete_embeddings=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o = (i for i in text2shape(test_paths))\n",
    "SIZE = sum(1 for _ in o)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "it = vertex_model_dataset.make_initializable_iterator()\n",
    "vertex_model_batch = it.get_next()\n",
    "iterator_init_op = it.initializer\n",
    "vertex_model_pred_dist = vertex_model(vertex_model_batch)\n",
    "vertex_model_loss = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) *\n",
    "    vertex_model_batch['vertices_flat_mask'])\n",
    "saver_vertex = tf.train.Saver(var_list=vertex_model.variables)\n",
    "with tf.Session() as sess:\n",
    "    saver_vertex.restore(sess, \"text_vertex_shapeglot/best-237\")\n",
    "    sess.run(iterator_init_op)\n",
    "    mean_loss = []\n",
    "    bits_per_vertex = []\n",
    "    for _ in tqdm(range(int(np.ceil(SIZE/16)))):\n",
    "        batch, loss =  sess.run((vertex_model_batch, vertex_model_loss))\n",
    "        mean_loss.append(loss/16)\n",
    "        num_vertices = 0\n",
    "        for b in range(batch['vertices'].shape[0]):\n",
    "            k = batch['vertices'][b]\n",
    "            k = k[~np.all(k == 0, axis=1)]\n",
    "            num_vertices+=k.shape[0]\n",
    "        bits_per_vertex.append(loss /  (np.log(2)*num_vertices))\n",
    "    print(np.array(mean_loss).mean())\n",
    "    print(np.array(bits_per_vertex).mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vertex_samples_val = vertex_model.sample(\n",
    "    16, context=vertex_model_batch, max_sample_length=800, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)\n",
    "with tf.Session() as sess:\n",
    "    saver_vertex.restore(sess, \"text_vertex_shapeglot/best-237\")\n",
    "    sess.run(iterator_init_op)\n",
    "    acc = []\n",
    "    for _ in tqdm(range(int(np.ceil(SIZE/16)))):\n",
    "        gt, pred = sess.run((vertex_model_batch, vertex_samples_val))\n",
    "        gt = gt['vertices']\n",
    "        pred = pred['vertices']*np.dstack([pred['vertices_mask'], pred['vertices_mask'], pred['vertices_mask']])\n",
    "        for b in range(pred.shape[0]):\n",
    "            k = gt[b]\n",
    "            k = data_utils.dequantize_verts(k[~np.all(k == 0, axis=1)])\n",
    "            gt_padded = np.pad(k, ((0, 800-k.shape[0]), (0, 0)))\n",
    "            acc.append((gt_padded == pred[b]).sum()/len(gt_padded.flatten()))\n",
    "    print(np.array(acc).mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
