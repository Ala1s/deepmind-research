{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T18:05:32.082630400Z",
     "start_time": "2023-07-13T18:05:24.717702700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 10:52:02.780247: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Hide TF deprecation messages\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import modules\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.020635900Z",
     "start_time": "2023-07-12T23:13:20.004836800Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tensorflow import summary as s\n",
    "# log_dir = \"logs/text_gen/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#summary_writer = s.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.161258600Z",
     "start_time": "2023-07-12T23:13:20.020635900Z"
    }
   },
   "outputs": [],
   "source": [
    "chair_meshes_paths = list(glob(\"chairs_ngon/*.obj\"))\n",
    "# chair_meshes_paths = [[\"Chair\", path] for path in chair_meshes_paths]\n",
    "# tables_meshes_paths = list(glob(\"tables_ngon/*.obj\"))\n",
    "# tables_meshes_paths = [[\"Table\", path] for path in tables_meshes_paths]\n",
    "# chair_meshes_paths.extend(tables_meshes_paths)\n",
    "# paths = chair_meshes_paths.copy()\n",
    "# random.shuffle(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chairs_split_dict.pickle\", 'rb') as f:\n",
    "    chairs_split_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"tables_split_dict.pickle\", 'rb') as f:\n",
    "#     tables_split_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_train = []\n",
    "chairs_val = []\n",
    "chairs_test = []\n",
    "for c in chair_meshes_paths: \n",
    "    try:\n",
    "        split = chairs_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "    except KeyError:\n",
    "#         print(c.split(\"/\")[-1].replace(\".obj\", \"\"))\n",
    "        continue\n",
    "    if split =='train':\n",
    "        chairs_train.append(c)\n",
    "    elif split =='val':\n",
    "        chairs_val.append(c)\n",
    "    else:\n",
    "        chairs_test.append(c)\n",
    "# print(len(chairs_train))\n",
    "# print(len(chairs_val))\n",
    "# print(len(chairs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_train = []\n",
    "# tables_val = []\n",
    "# tables_test = []\n",
    "# for c in tables_meshes_paths:\n",
    "#     try:\n",
    "#         split = tables_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "#     except KeyError:\n",
    "# #         print(c.split(\"/\")[-1].replace(\".obj\", \"\"))\n",
    "#         continue\n",
    "#     if split =='train':\n",
    "#         tables_train.append(c)\n",
    "#     elif split =='val':\n",
    "#         tables_val.append(c)\n",
    "#     else:\n",
    "#         tables_test.append(c)\n",
    "# # print(len(tables_train))\n",
    "# # print(len(tables_val))\n",
    "# # print(len(tables_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chairs_train.extend(tables_train)\n",
    "train_paths = chairs_train.copy()\n",
    "random.shuffle(train_paths)\n",
    "# train_paths = train_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chairs_val.extend(tables_val)\n",
    "val_paths = chairs_val.copy()\n",
    "random.shuffle(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = len(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.346341600Z",
     "start_time": "2023-07-12T23:13:20.161258600Z"
    }
   },
   "outputs": [],
   "source": [
    "captions = pd.read_csv(\"captions_tablechair.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.449846300Z",
     "start_time": "2023-07-12T23:13:20.346341600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9483664640781337"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = []\n",
    "for c in captions['description'].values:\n",
    "    max_length.append(len(c.split(\" \")))\n",
    "max_length = np.array(max_length)\n",
    "(max_length < 30).sum()/len(captions['description'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions=[]\n",
    "for index, row in captions.iterrows():\n",
    "    try:\n",
    "        # if row[\"category\"]==\"Table\":\n",
    "        #     if tables_split_dict[row[\"modelId\"]]=='train':\n",
    "        #         train_captions.append(row['description'])\n",
    "        if row[\"category\"]==\"Chair\":\n",
    "            if chairs_split_dict[row[\"modelId\"]]=='train':\n",
    "                train_captions.append(row['description'])\n",
    "    except KeyError:\n",
    "        continue\n",
    "# print(len(train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.598342800Z",
     "start_time": "2023-07-12T23:13:20.449846300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.613926100Z",
     "start_time": "2023-07-12T23:13:21.598342800Z"
    }
   },
   "outputs": [],
   "source": [
    "def text2shape(paths, captions, tokenizer):\n",
    "    for path in paths:\n",
    "        # with open(path, 'rb') as obj_file:\n",
    "        mesh_dict = data_utils.load_process_mesh(path)\n",
    "#         mesh_dict['class_label'] = 18 if cls==\"Chair\" else 49\n",
    "        if len(mesh_dict['vertices'])>500:\n",
    "            continue\n",
    "        if len(mesh_dict['faces'])>2600:\n",
    "            continue\n",
    "        # mesh_dict = random_scaling(mesh_dict)\n",
    "        try:\n",
    "            text = captions[captions[\"modelId\"]==path.split(\"/\")[-1].replace(\".obj\", \"\")].sample(n=1)[\"description\"].values[0]\n",
    "        except:\n",
    "            continue\n",
    "        text = text.lower().replace(\"the\", '').replace(\"a\", '').replace(\"of\", '').replace(\"for\", '').replace(\"and\", '').replace(\"to\", '').replace(\"in\", '')\n",
    "        text = \" \".join(text.split(\" \")[:max_length])\n",
    "        text = tokenizer.texts_to_sequences([text])[0]\n",
    "        mesh_dict['text_feature'] = np.pad(text, (0,max_length-len(text)))\n",
    "        yield mesh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.660455100Z",
     "start_time": "2023-07-12T23:13:21.613926100Z"
    }
   },
   "outputs": [],
   "source": [
    "Text2ShapeDataset = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(train_paths, captions, tk),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "#             'class_label': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "#             'class_label': tf.TensorShape(()),\n",
    "            'text_feature':tf.TensorShape(max_length)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.906000800Z",
     "start_time": "2023-07-12T23:13:21.706445600Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model_dataset = data_utils.make_vertex_model_dataset(Text2ShapeDataset, apply_random_shift=False)\n",
    "vertex_model_dataset = vertex_model_dataset.repeat()\n",
    "vertex_model_dataset = vertex_model_dataset.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset.output_shapes)\n",
    "vertex_model_dataset = vertex_model_dataset.prefetch(1)\n",
    "it = vertex_model_dataset.make_initializable_iterator()\n",
    "vertex_model_batch = it.get_next()\n",
    "iterator_init_op_train = it.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 10:52:26.325487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-07-23 10:52:26.437790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.74\n",
      "pciBusID: 0000:65:00.0\n",
      "2023-07-23 10:52:26.437854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-23 10:52:26.472236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-07-23 10:52:26.476093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-23 10:52:26.476779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-23 10:52:26.478564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-07-23 10:52:26.479992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-07-23 10:52:26.480523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-07-23 10:52:26.480920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-07-23 10:52:26.517324: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 3899935000 Hz\n",
      "2023-07-23 10:52:26.518727: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ad0e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-23 10:52:26.518762: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-07-23 10:52:26.614908: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47d7290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-23 10:52:26.614954: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-07-23 10:52:26.615516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.74\n",
      "pciBusID: 0000:65:00.0\n",
      "2023-07-23 10:52:26.615584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-23 10:52:26.615634: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-07-23 10:52:26.615664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-23 10:52:26.615692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-23 10:52:26.615721: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-07-23 10:52:26.615749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-07-23 10:52:26.615778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-07-23 10:52:26.616325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-07-23 10:52:26.616383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-23 10:52:26.881983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-07-23 10:52:26.882012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-07-23 10:52:26.882017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-07-23 10:52:26.882392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22054 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6)\n",
      "/cluster/51/ala1s/deepmind-research/polygen/data_utils.py:162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(vertices), np.array(faces)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'vertices': array([[[164, 167,  18],\n",
       "          [173, 167,  18],\n",
       "          [164, 174,  18],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       "  \n",
       "         [[ 65,  65,  57],\n",
       "          [ 70,  65,  57],\n",
       "          [ 65,  67,  57],\n",
       "          ...,\n",
       "          [189, 130, 196],\n",
       "          [181, 130, 197],\n",
       "          [184, 130, 197]],\n",
       "  \n",
       "         [[ 78,  60,  42],\n",
       "          [ 79,  60,  42],\n",
       "          [194,  60,  42],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[ 84,  84,  15],\n",
       "          [ 93,  84,  15],\n",
       "          [143,  84,  15],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       "  \n",
       "         [[ 81,  63,  46],\n",
       "          [ 91,  63,  46],\n",
       "          [151,  63,  46],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       "  \n",
       "         [[ 71,  47,  46],\n",
       "          [ 80,  47,  46],\n",
       "          [174,  47,  46],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]]], dtype=int32),\n",
       "  'faces': array([[  2,   4,   5, ...,   0,   0,   0],\n",
       "         [  2,   3,   5, ...,   0,   0,   0],\n",
       "         [  2,   4,   3, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  2,   3,  18, ...,   0,   0,   0],\n",
       "         [  2,   6,   7, ..., 293, 292,   0],\n",
       "         [  2,   6,   7, ...,   0,   0,   0]], dtype=int32),\n",
       "  'text_feature': array([[  24, 2804,    4, 1781,  325, 2747, 1684, 2196,    8, 1673, 1781,\n",
       "           259,  222,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [2804,    4,  162,    8,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  13,    6, 4205,   22,   14,    6,   10,   13,   60,   49,  259,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  29, 2804,    4,   50, 1921,   24,    8,  114,   42,   23,   76,\n",
       "           120,  632,  256,    4,   76, 1086,   13,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  29, 2804,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 259,  113, 2804,    4,   26, 4205,   22, 2747,   18,    8,   87,\n",
       "            10,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  23,   90,   91,  284, 2804,    4,   10,   14,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  10,    9,    4,   18,    8, 3555,   63,   10,   23,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32),\n",
       "  'vertices_flat': array([[ 19, 168, 165, ...,   0,   0,   0],\n",
       "         [ 58,  66,  66, ..., 131, 185,   0],\n",
       "         [ 43,  61,  79, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 16,  85,  85, ...,   0,   0,   0],\n",
       "         [ 47,  64,  82, ...,   0,   0,   0],\n",
       "         [ 47,  48,  72, ...,   0,   0,   0]], dtype=int32),\n",
       "  'vertices_flat_mask': array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([iterator_init_op_train])\n",
    "    vv = sess.run([vertex_model_batch])\n",
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text2ShapeDatasetVal = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(val_paths, captions, tk),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "#             'class_label': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "#             'class_label': tf.TensorShape(()),\n",
    "            'text_feature':tf.TensorShape(max_length)})\n",
    "vertex_model_dataset_val = data_utils.make_vertex_model_dataset(Text2ShapeDatasetVal, apply_random_shift=False)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.repeat()\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset_val.output_shapes)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.prefetch(1)\n",
    "itv = vertex_model_dataset_val.make_initializable_iterator()\n",
    "vertex_model_batch_val = itv.get_next()\n",
    "iterator_init_op_val = itv.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = (i for i in text2shape(train_paths, captions, tk))\n",
    "TRAIN_SIZE = sum(1 for _ in o)\n",
    "o = (i for i in text2shape(val_paths, captions, tk))\n",
    "VAL_SIZE = sum(1 for _ in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1805"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:39.111517Z",
     "start_time": "2023-07-12T23:13:21.906000800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 5033 words (1171 misses)\n"
     ]
    }
   ],
   "source": [
    "vertex_model = modules.TextToVertexModel(\n",
    "    decoder_config=dict(\n",
    "      hidden_size=128,\n",
    "      fc_size=512,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=24,\n",
    "      dropout_rate=0.4,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True\n",
    "      ),\n",
    "    path_to_embeddings=\"glove.6B/glove.6B.100d.txt\",\n",
    "    embedding_dims = 100,\n",
    "    quantization_bits=8,\n",
    "    tokenizer=tk,\n",
    "    max_num_input_verts=500,  # number of vertices in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "    use_discrete_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.650342600Z",
     "start_time": "2023-07-12T23:13:39.111517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vertices': <tf.Tensor 'IteratorGetNext:2' shape=(?, ?, 3) dtype=int32>, 'faces': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int32>, 'text_feature': <tf.Tensor 'IteratorGetNext:1' shape=(?, 30) dtype=int32>, 'vertices_flat': <tf.Tensor 'IteratorGetNext:3' shape=(?, ?) dtype=int32>, 'vertices_flat_mask': <tf.Tensor 'IteratorGetNext:4' shape=(?, ?) dtype=float32>}\n",
      "tfp.distributions.Categorical(\"vertex_model_2/vertex_model/create_dist/Categorical/\", batch_shape=[?, ?], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vertex_model_pred_dist = vertex_model(vertex_model_batch, is_training=True)\n",
    "vertex_model_loss = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) *\n",
    "    vertex_model_batch['vertices_flat_mask'])\n",
    "vertex_samples = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch, max_sample_length=500, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)\n",
    "\n",
    "print(vertex_model_batch)\n",
    "print(vertex_model_pred_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model_pred_dist_val = vertex_model(vertex_model_batch_val)\n",
    "vertex_model_loss_val = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist_val.log_prob(vertex_model_batch_val['vertices_flat']) *\n",
    "    vertex_model_batch_val['vertices_flat_mask'])\n",
    "vertex_samples_val = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch_val, max_sample_length=500, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"only_text_vars.pickle\", 'rb') as f:\n",
    "#     only_text_vars = pickle.load(f)\n",
    "\n",
    "# text_vars = []\n",
    "# for var in vertex_model.variables:\n",
    "#     if '/'.join((var._variable._name).split('/')[1:]) in only_text_vars:\n",
    "#         text_vars.append(var)\n",
    "# text_vars=tuple(text_vars)\n",
    "# # print(text_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.794703100Z",
     "start_time": "2023-07-12T23:13:57.779077200Z"
    }
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(\"pretrained_vars.pickle\", 'rb') as f:\n",
    "#    common_vars = pickle.load(f)\n",
    "\n",
    "#pretrained_vars = []\n",
    "#for var in vertex_model.variables:\n",
    "##    if '/'.join((var._variable._name).split('/')[1:]) in common_vars:\n",
    "#        pretrained_vars.append(var)\n",
    "#pretrained_vars=tuple(pretrained_vars)\n",
    "# print(pretrained_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.810357600Z",
     "start_time": "2023-07-12T23:13:57.794703100Z"
    }
   },
   "outputs": [],
   "source": [
    "#vertex_model_saver = tf.train.Saver(var_list=pretrained_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:14:13.344725100Z",
     "start_time": "2023-07-12T23:13:57.825948300Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create face model\n",
    "# face_model = modules.FaceModel(\n",
    "#       encoder_config=dict(\n",
    "#       hidden_size=128,\n",
    "#       fc_size=512,\n",
    "#       num_heads=8,\n",
    "#       layer_norm=True,\n",
    "#       num_layers=10,\n",
    "#       dropout_rate=0.2,\n",
    "#       re_zero=True,\n",
    "#       memory_efficient=True,\n",
    "#       ),\n",
    "#   decoder_config=dict(\n",
    "#       hidden_size=128,\n",
    "#       fc_size=512,\n",
    "#       num_heads=8,\n",
    "#       layer_norm=True,\n",
    "#       num_layers=14,\n",
    "#       dropout_rate=0.2,\n",
    "#       re_zero=True,\n",
    "#       memory_efficient=True,\n",
    "#       ),\n",
    "#     class_conditional=False,\n",
    "#     max_seq_length=1000, # number of faces in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "#     quantization_bits=8,\n",
    "#     decoder_cross_attention=True,\n",
    "#     use_discrete_vertex_embeddings=True,\n",
    "# )\n",
    "\n",
    "face_module_config=dict(\n",
    "  encoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=10,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "  decoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=14,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "  class_conditional=False,\n",
    "  decoder_cross_attention=True,\n",
    "  use_discrete_vertex_embeddings=True,\n",
    "  max_seq_length=8000,\n",
    "  )\n",
    "face_model=modules.FaceModel(**face_module_config)\n",
    "\n",
    "# face_model_dataset = data_utils.make_face_model_dataset(\n",
    "#     Text2ShapeDataset, apply_random_shift=True)\n",
    "# face_model_dataset = face_model_dataset.repeat()\n",
    "# face_model_dataset = face_model_dataset.padded_batch(\n",
    "#     BATCH_SIZE, padded_shapes=face_model_dataset.output_shapes)\n",
    "# face_model_dataset = face_model_dataset.prefetch(1)\n",
    "# itf = face_model_dataset.make_initializable_iterator()\n",
    "# face_model_batch = itf.get_next()\n",
    "# f_iterator_init_op_train = itf.initializer\n",
    "\n",
    "# face_model_pred_dist = face_model(face_model_batch)\n",
    "# face_model_loss = -tf.reduce_sum(\n",
    "#     face_model_pred_dist.log_prob(face_model_batch['faces']) *\n",
    "#     face_model_batch['faces_mask'])\n",
    "face_samples = face_model.sample(\n",
    "    context=vertex_samples, max_sample_length=2000, top_p=0.95,\n",
    "    only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_model_dataset_val = data_utils.make_face_model_dataset(\n",
    "#     Text2ShapeDatasetVal, apply_random_shift=False)\n",
    "# face_model_dataset_val  = face_model_dataset_val.repeat()\n",
    "# face_model_dataset_val  = face_model_dataset_val.padded_batch(\n",
    "#     BATCH_SIZE, padded_shapes=face_model_dataset_val.output_shapes)\n",
    "# face_model_dataset_val = face_model_dataset_val.prefetch(1)\n",
    "# itfv = face_model_dataset_val.make_initializable_iterator()\n",
    "# face_model_batch_val = itfv.get_next()\n",
    "# f_iterator_init_op_val = itfv.initializer\n",
    "\n",
    "# face_model_pred_dist_val = face_model(face_model_batch_val)\n",
    "# face_model_loss_val = -tf.reduce_sum(\n",
    "#     face_model_pred_dist_val.log_prob(face_model_batch_val['faces']) *\n",
    "#     face_model_batch_val['faces_mask'])\n",
    "face_samples_val = face_model.sample(\n",
    "    context=vertex_samples_val, max_sample_length=2000, top_p=0.95,\n",
    "    only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:14:13.360755400Z",
     "start_time": "2023-07-12T23:14:13.344725100Z"
    }
   },
   "outputs": [],
   "source": [
    "face_model_saver = tf.train.Saver(var_list=face_model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 10:54:32.653175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.74\n",
      "pciBusID: 0000:65:00.0\n",
      "2023-07-23 10:54:32.653221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-23 10:54:32.653243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-07-23 10:54:32.653250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-23 10:54:32.653256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-23 10:54:32.653263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-07-23 10:54:32.653269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-07-23 10:54:32.653275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-07-23 10:54:32.653443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-07-23 10:54:32.653463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-07-23 10:54:32.653466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-07-23 10:54:32.653468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-07-23 10:54:32.653639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22054 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/226 [00:00<?, ?it/s]2023-07-23 10:55:34.648916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "/cluster/51/ala1s/deepmind-research/polygen/data_utils.py:162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(vertices), np.array(faces)\n",
      "2023-07-23 10:55:35.773992: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "100%|█████████████████████████████████████████████████| 226/226 [04:32<00:00,  1.21s/it, loss=4947.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5170.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:14<00:00,  2.97it/s, loss=3685.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4424.426\n",
      "- Found new best vertex model, saving in text_vertex/best-98\n",
      "Epoch 99/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:26<00:00,  1.09it/s, loss=6550.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5221.6914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.81it/s, loss=3885.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4448.12\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=4966.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5105.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.79it/s, loss=3868.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4419.718\n",
      "- Found new best vertex model, saving in text_vertex/best-100\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:24<00:00,  1.10it/s, loss=4149.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4971.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.83it/s, loss=3841.773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4450.399\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.12it/s, loss=5924.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5133.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.85it/s, loss=3834.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4422.902\n",
      "Epoch 103/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=5361.548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5112.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.93it/s, loss=3933.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4394.3223\n",
      "- Found new best vertex model, saving in text_vertex/best-103\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5614.370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5064.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.92it/s, loss=3941.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4394.7344\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.12it/s, loss=5896.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4984.174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.92it/s, loss=3788.620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4375.716\n",
      "- Found new best vertex model, saving in text_vertex/best-105\n",
      "Epoch 106/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=5630.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5085.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.91it/s, loss=3687.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4320.0303\n",
      "- Found new best vertex model, saving in text_vertex/best-106\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:20<00:00,  1.13it/s, loss=5585.632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5096.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.95it/s, loss=3688.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4330.7656\n",
      "Epoch 108/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=4849.960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5093.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.85it/s, loss=3870.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4377.1294\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=6353.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5071.8794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.89it/s, loss=3913.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4376.06\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.12it/s, loss=6512.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4992.4575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.93it/s, loss=3700.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4321.731\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5828.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5118.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.92it/s, loss=3470.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4358.2305\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5380.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5065.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  5.06it/s, loss=3954.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4304.3633\n",
      "- Found new best vertex model, saving in text_vertex/best-112\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.14it/s, loss=4970.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5008.0884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.90it/s, loss=3517.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4319.275\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=4463.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4983.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.89it/s, loss=3977.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4311.9585\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=3992.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4959.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.93it/s, loss=3619.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4305.0225\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.14it/s, loss=4464.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4966.5913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.96it/s, loss=3798.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4317.049\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=6443.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4990.8945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.82it/s, loss=4515.912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4360.4106\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:20<00:00,  1.13it/s, loss=5722.759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 5032.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.89it/s, loss=3591.090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4259.0693\n",
      "- Found new best vertex model, saving in text_vertex/best-118\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=5278.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4995.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.68it/s, loss=3865.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4326.6865\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=5612.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4910.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.84it/s, loss=3823.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4301.5303\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5847.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4946.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.88it/s, loss=3736.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4284.5166\n",
      "Epoch 122/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5432.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4969.1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.93it/s, loss=3687.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4301.121\n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=5787.090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4964.1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.87it/s, loss=3793.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4289.389\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=6286.977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4863.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.96it/s, loss=3497.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4254.1694\n",
      "- Found new best vertex model, saving in text_vertex/best-124\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=5131.590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4910.7515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.92it/s, loss=3901.220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4271.222\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=6715.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4949.3774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.79it/s, loss=3722.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4244.4194\n",
      "- Found new best vertex model, saving in text_vertex/best-126\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=4467.620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4900.2524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.89it/s, loss=3746.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4221.1997\n",
      "- Found new best vertex model, saving in text_vertex/best-127\n",
      "Epoch 128/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=5317.841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4946.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.88it/s, loss=3780.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4272.4165\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:20<00:00,  1.12it/s, loss=6194.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4926.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.97it/s, loss=3718.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4245.4414\n",
      "Epoch 130/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:23<00:00,  1.11it/s, loss=5600.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4810.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.88it/s, loss=3428.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4225.868\n",
      "Epoch 131/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=5603.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4927.5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.91it/s, loss=3519.710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4233.962\n",
      "Epoch 132/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=5563.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4986.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.87it/s, loss=3923.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4215.2666\n",
      "- Found new best vertex model, saving in text_vertex/best-132\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.12it/s, loss=6681.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4874.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.81it/s, loss=3798.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4243.35\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:20<00:00,  1.13it/s, loss=5349.742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4836.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.89it/s, loss=3670.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4246.141\n",
      "Epoch 135/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.12it/s, loss=5573.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4828.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.86it/s, loss=4003.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4204.607\n",
      "- Found new best vertex model, saving in text_vertex/best-135\n",
      "Epoch 136/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:20<00:00,  1.13it/s, loss=5534.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4881.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.88it/s, loss=3606.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4207.2407\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:18<00:00,  1.14it/s, loss=5102.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4811.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.90it/s, loss=3749.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4216.778\n",
      "Epoch 138/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:21<00:00,  1.12it/s, loss=6574.930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4875.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.90it/s, loss=3426.480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4215.085\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:19<00:00,  1.13it/s, loss=6257.948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4930.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.78it/s, loss=3644.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4194.5093\n",
      "- Found new best vertex model, saving in text_vertex/best-139\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:17<00:00,  1.14it/s, loss=4341.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4835.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.96it/s, loss=3814.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4205.607\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 226/226 [03:22<00:00,  1.11it/s, loss=6554.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss vertex: 4824.2886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 42/42 [00:08<00:00,  4.95it/s, loss=3532.973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval loss vertex: 4220.7607\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████▏                                 | 73/226 [01:02<01:58,  1.30it/s, loss=5739.374]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "last_saver_vertex = tf.train.Saver(var_list=vertex_model.variables) # will keep last 5 epochs\n",
    "best_saver_vertex = tf.train.Saver(var_list=vertex_model.variables, max_to_keep=2)  # only keep 1 best checkpoint (best on eval)\n",
    "\n",
    "# last_saver_faces = tf.train.Saver(var_list=face_model.variables) # will keep last 5 epochs\n",
    "# best_saver_faces = tf.train.Saver(var_list=face_model.variables, max_to_keep=2)  # only keep 1 best checkpoint (best on eval)\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "learning_rate = 3e-4\n",
    "training_steps = 500\n",
    "check_step_metrics = 50\n",
    "check_step_samples = 50\n",
    "EPOCHS = 500\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#vertex_model_optim_op = optimizer.minimize(vertex_model_loss, var_list=text_vars)\n",
    "vertex_model_optim_op = optimizer.minimize(vertex_model_loss, var_list=vertex_model.variables)\n",
    "best_v_loss = 4194.5093 #4608.057 #float('inf')\n",
    "# face_model_optim_op = optimizer.minimize(face_model_loss, var_list=face_model.variables)\n",
    "# best_f_loss = float('inf')\n",
    "#gstep = 0\n",
    "# Training loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    last_saver_vertex.restore(sess, \"text_vertex/last-141\")\n",
    "    # best_saver_faces.restore(sess, 'text_f/best-4')\n",
    "    # best_saver_faces.restore(sess, \"text_f/best-9\")\n",
    "    face_model_saver.restore(sess, \"face_model/model\")\n",
    "\n",
    "    for e in range(142, EPOCHS):\n",
    "        print(\"Epoch {}/{}\".format(e + 1, EPOCHS))\n",
    "        num_steps = (TRAIN_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_train)\n",
    "        # sess.run(f_iterator_init_op_train)\n",
    "        t = trange(num_steps)\n",
    "        loss_values = []\n",
    "        # loss_values_f=[]\n",
    "        for i in t:\n",
    "            sess.run(vertex_model_optim_op)\n",
    "            # sess.run(face_model_optim_op)\n",
    "\n",
    "            loss_val = sess.run(vertex_model_loss)\n",
    "            loss_values.append(loss_val)\n",
    "            \n",
    "            # loss_val_f = sess.run(face_model_loss)\n",
    "            # loss_values_f.append(loss_val_f)\n",
    " \n",
    "            # Log the loss in the tqdm progress bar\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        # mean_loss_f = np.array(loss_values_f).mean()\n",
    "        print(\"- Train loss vertex: \" + str(mean_loss)) \n",
    "        # print(\"- Train loss faces: \" + str(mean_loss_f))\n",
    "        \n",
    "        last_save_path = os.path.join(\"text_vertex\", 'last')\n",
    "        last_saver_vertex.save(sess, last_save_path, global_step=e + 1)\n",
    "        # last_save_path = os.path.join(\"text_f\", 'last')\n",
    "        # last_saver_faces.save(sess, last_save_path, global_step=e + 1)\n",
    "    \n",
    "        num_steps = (VAL_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_val)\n",
    "        # sess.run(f_iterator_init_op_val)\n",
    "        loss_values = []\n",
    "        # loss_values_f = []\n",
    "        t = trange(num_steps)\n",
    "        for i in t:\n",
    "            loss_val = sess.run(vertex_model_loss_val)\n",
    "            loss_values.append(loss_val)\n",
    "            # # loss_val_f = sess.run(face_model_loss_val)\n",
    "            # loss_values_f.append(loss_val_f)\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        # mean_loss_f = np.array(loss_values_f).mean()\n",
    "        print(\"- Eval loss vertex: \" + str(mean_loss)) \n",
    "        # print(\"- Eval loss faces: \" + str(mean_loss_f))\n",
    " \n",
    "        if mean_loss<=best_v_loss:\n",
    "            best_v_loss = mean_loss\n",
    "            best_save_path = os.path.join(\"text_vertex\", 'best')\n",
    "            best_save_path = best_saver_vertex.save(sess, best_save_path, global_step=e + 1)\n",
    "            print(\"- Found new best vertex model, saving in {}\".format(best_save_path))\n",
    "        # if mean_loss_f<=best_f_loss:\n",
    "        #     best_f_loss = mean_loss_f\n",
    "        #     best_save_path = os.path.join(\"text_f\", 'best')\n",
    "        #     best_save_path = best_saver_faces.save(sess, best_save_path, global_step=e + 1)\n",
    "        #     print(\"- Found new best face model, saving in {}\".format(best_save_path))\n",
    "               \n",
    "        #SAmples\n",
    "        if e>125 and e % check_step_samples==0:\n",
    "            sess.run(iterator_init_op_val)\n",
    "            # sess.run(f_iterator_init_op_val)\n",
    "            v_samples_np, f_samples_np, b_np = sess.run((vertex_samples_val, face_samples_val, vertex_model_batch_val))\n",
    "            mesh_list = []\n",
    "            for n in range(BATCH_SIZE):\n",
    "                mesh_list.append({\n",
    "                    'vertices': v_samples_np['vertices'][n][:v_samples_np['num_vertices'][n]],\n",
    "                    'faces': data_utils.unflatten_faces(\n",
    "                        f_samples_np['faces'][n][:f_samples_np['num_face_indices'][n]])\n",
    "                    })\n",
    "            data_utils.plot_meshes(mesh_list, ax_lims=0.5)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last1",
   "language": "python",
   "name": "last1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
