{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T18:05:32.082630400Z",
     "start_time": "2023-07-13T18:05:24.717702700Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Hide TF deprecation messages\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import modules\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.161258600Z",
     "start_time": "2023-07-12T23:13:20.020635900Z"
    }
   },
   "outputs": [],
   "source": [
    "chair_meshes_paths = list(glob(\"chairs_ngon/*.obj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chairs_split_dict.pickle\", 'rb') as f:\n",
    "    chairs_split_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_train = []\n",
    "chairs_val = []\n",
    "chairs_test = []\n",
    "for c in chair_meshes_paths: \n",
    "    try:\n",
    "        split = chairs_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "    except KeyError:\n",
    "#         print(c.split(\"/\")[-1].replace(\".obj\", \"\"))\n",
    "        continue\n",
    "    if split =='train':\n",
    "        chairs_train.append(c)\n",
    "    elif split =='val':\n",
    "        chairs_val.append(c)\n",
    "    else:\n",
    "        chairs_test.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = chairs_train.copy()\n",
    "random.shuffle(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_paths = chairs_val.copy()\n",
    "random.shuffle(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chair_to_text.pickle', 'rb') as f:\n",
    "    chair_to_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_int.pickle', 'rb') as f:\n",
    "    word_to_int = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('int_to_word.pickle', 'rb') as f:\n",
    "    int_to_word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.449846300Z",
     "start_time": "2023-07-12T23:13:20.346341600Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "count = 0\n",
    "for key, val in chair_to_text.items():\n",
    "    for t in val:\n",
    "        lengths.append(len(t))\n",
    "        count+=1\n",
    "max_length = np.array(lengths)\n",
    "(max_length < 30).sum()/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.613926100Z",
     "start_time": "2023-07-12T23:13:21.598342800Z"
    }
   },
   "outputs": [],
   "source": [
    "def text2shape(paths):\n",
    "    for path in paths:\n",
    "        mesh_dict = data_utils.load_process_mesh(path)\n",
    "        if len(mesh_dict['vertices'])>800:\n",
    "            continue\n",
    "        if len(mesh_dict['faces'])>2800:\n",
    "            continue\n",
    "        try:\n",
    "            texts =  chair_to_text[path.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        text = np.random.choice(texts)[:max_length]\n",
    "        mesh_dict['text_feature'] = np.pad(text, (0,max_length-len(text)))\n",
    "        yield mesh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.660455100Z",
     "start_time": "2023-07-12T23:13:21.613926100Z"
    }
   },
   "outputs": [],
   "source": [
    "Text2ShapeDataset = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(train_paths),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "            'text_feature':tf.TensorShape(max_length)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.906000800Z",
     "start_time": "2023-07-12T23:13:21.706445600Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model_dataset = data_utils.make_vertex_model_dataset(Text2ShapeDataset, apply_random_shift=True)\n",
    "vertex_model_dataset = vertex_model_dataset.repeat()\n",
    "vertex_model_dataset = vertex_model_dataset.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset.output_shapes)\n",
    "vertex_model_dataset = vertex_model_dataset.prefetch(1)\n",
    "it = vertex_model_dataset.make_initializable_iterator()\n",
    "vertex_model_batch = it.get_next()\n",
    "iterator_init_op_train = it.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text2ShapeDatasetVal = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(val_paths),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "            'text_feature':tf.TensorShape(max_length)})\n",
    "vertex_model_dataset_val = data_utils.make_vertex_model_dataset(Text2ShapeDatasetVal, apply_random_shift=False)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.repeat()\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset_val.output_shapes)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.prefetch(1)\n",
    "itv = vertex_model_dataset_val.make_initializable_iterator()\n",
    "vertex_model_batch_val = itv.get_next()\n",
    "iterator_init_op_val = itv.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = (i for i in text2shape(train_paths))\n",
    "TRAIN_SIZE = sum(1 for _ in o)\n",
    "o = (i for i in text2shape(val_paths))\n",
    "VAL_SIZE = sum(1 for _ in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:39.111517Z",
     "start_time": "2023-07-12T23:13:21.906000800Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model = modules.TextToVertexModel(\n",
    "    decoder_config=dict(\n",
    "      hidden_size=128,\n",
    "      fc_size=512,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=24,\n",
    "      dropout_rate=0.4,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True\n",
    "      ),\n",
    "    path_to_embeddings=\"glove/glove.6B.100d.txt\",\n",
    "    embedding_dims = 100,\n",
    "    quantization_bits=8,\n",
    "    vocab=word_to_int,\n",
    "    max_num_input_verts=800,  # number of vertices in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "    use_discrete_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.650342600Z",
     "start_time": "2023-07-12T23:13:39.111517Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model_pred_dist = vertex_model(vertex_model_batch, is_training=True)\n",
    "vertex_model_loss = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) *\n",
    "    vertex_model_batch['vertices_flat_mask'])\n",
    "vertex_samples = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch, max_sample_length=800, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model_pred_dist_val = vertex_model(vertex_model_batch_val)\n",
    "vertex_model_loss_val = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist_val.log_prob(vertex_model_batch_val['vertices_flat']) *\n",
    "    vertex_model_batch_val['vertices_flat_mask'])\n",
    "vertex_samples_val = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch_val, max_sample_length=800, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_module_config=dict(\n",
    "  encoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=10,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "  decoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=14,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "  class_conditional=False,\n",
    "  decoder_cross_attention=True,\n",
    "  use_discrete_vertex_embeddings=True,\n",
    "  max_seq_length=8000,\n",
    "  )\n",
    "face_model=modules.FaceModel(**face_module_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_samples_val = face_model.sample(\n",
    "    context=vertex_samples_val, max_sample_length=2800, top_p=0.95,\n",
    "    only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:14:13.360755400Z",
     "start_time": "2023-07-12T23:14:13.344725100Z"
    }
   },
   "outputs": [],
   "source": [
    "face_model_saver = tf.train.Saver(var_list=face_model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#borrowed from https://github.com/optas/shapeglot\n",
    "def token_ints_to_sentence(tokens, int_to_word):\n",
    "    text = [int_to_word[i] for i in tokens]\n",
    "    text = ' '.join(text)\n",
    "    stop = text.find('<EOS>')\n",
    "    if stop == -1:\n",
    "        stop = len(text)\n",
    "    text = text[:stop]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "last_saver_vertex = tf.train.Saver(var_list=vertex_model.variables) # will keep last 5 epochs\n",
    "best_saver_vertex = tf.train.Saver(var_list=vertex_model.variables, max_to_keep=2)  # only keep 1 best\n",
    "\n",
    "learning_rate = 3e-4\n",
    "training_steps = 500\n",
    "check_step_metrics = 50\n",
    "check_step_samples = 50\n",
    "EPOCHS = 500\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "vertex_model_optim_op = optimizer.minimize(vertex_model_loss, var_list=vertex_model.variables)\n",
    "best_v_loss = float('inf')\n",
    "# Training loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    face_model_saver.restore(sess, \"face_model/model\")\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        print(\"Epoch {}/{}\".format(e + 1, EPOCHS))\n",
    "        num_steps = (TRAIN_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_train)\n",
    "        t = trange(num_steps)\n",
    "        loss_values = []\n",
    "        for i in t:\n",
    "            sess.run(vertex_model_optim_op)\n",
    "\n",
    "            loss_val = sess.run(vertex_model_loss)\n",
    "            loss_values.append(loss_val)\n",
    " \n",
    "            # Log the loss in the tqdm progress bar\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        print(\"- Train loss vertex: \" + str(mean_loss))\n",
    "        \n",
    "        last_save_path = os.path.join(\"text_vertex_shapeglot\", 'last')\n",
    "        last_saver_vertex.save(sess, last_save_path, global_step=e + 1)\n",
    "    \n",
    "        num_steps = (VAL_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_val)\n",
    "        loss_values = []\n",
    "        t = trange(num_steps)\n",
    "        for i in t:\n",
    "            loss_val = sess.run(vertex_model_loss_val)\n",
    "            loss_values.append(loss_val)\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        print(\"- Eval loss vertex: \" + str(mean_loss))\n",
    " \n",
    "        if mean_loss<=best_v_loss:\n",
    "            best_v_loss = mean_loss\n",
    "            best_save_path = os.path.join(\"text_vertex_shapeglot\", 'best')\n",
    "            best_save_path = best_saver_vertex.save(sess, best_save_path, global_step=e + 1)\n",
    "            print(\"- Found new best vertex model, saving in {}\".format(best_save_path))\n",
    "\n",
    "        if e>150 and e % check_step_samples==0:\n",
    "            sess.run(iterator_init_op_val)\n",
    "            v_samples_np, f_samples_np, b_np = sess.run((vertex_samples_val, face_samples_val, vertex_model_batch_val))\n",
    "            mesh_list = []\n",
    "            for n in range(BATCH_SIZE):\n",
    "                print(token_ints_to_sentence(b_np['text_feature'][n], int_to_word))\n",
    "                mesh_list.append({\n",
    "                    'vertices': v_samples_np['vertices'][n][:v_samples_np['num_vertices'][n]],\n",
    "                    'faces': data_utils.unflatten_faces(\n",
    "                        f_samples_np['faces'][n][:f_samples_np['num_face_indices'][n]])\n",
    "                    })\n",
    "            data_utils.plot_meshes(mesh_list, ax_lims=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml43d",
   "language": "python",
   "name": "ml43d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
