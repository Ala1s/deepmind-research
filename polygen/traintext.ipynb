{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T18:05:32.082630400Z",
     "start_time": "2023-07-13T18:05:24.717702700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 23:39:50.977391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Hide TF deprecation messages\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import modules\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.020635900Z",
     "start_time": "2023-07-12T23:13:20.004836800Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tensorflow import summary as s\n",
    "log_dir = \"logs/text_gen/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# summary_writer = s.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.161258600Z",
     "start_time": "2023-07-12T23:13:20.020635900Z"
    }
   },
   "outputs": [],
   "source": [
    "chair_meshes_paths = list(glob(\"chairs_ngon/*.obj\"))\n",
    "# chair_meshes_paths = [[\"Chair\", path] for path in chair_meshes_paths]\n",
    "tables_meshes_paths = list(glob(\"tables_ngon/*.obj\"))\n",
    "# tables_meshes_paths = [[\"Table\", path] for path in tables_meshes_paths]\n",
    "# chair_meshes_paths.extend(tables_meshes_paths)\n",
    "# paths = chair_meshes_paths.copy()\n",
    "# random.shuffle(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chairs_split_dict.pickle\", 'rb') as f:\n",
    "    chairs_split_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tables_split_dict.pickle\", 'rb') as f:\n",
    "    tables_split_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_train = []\n",
    "chairs_val = []\n",
    "chairs_test = []\n",
    "for c in chair_meshes_paths: \n",
    "    try:\n",
    "        split = chairs_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "    except KeyError:\n",
    "#         print(c.split(\"/\")[-1].replace(\".obj\", \"\"))\n",
    "        continue\n",
    "    if split =='train':\n",
    "        chairs_train.append(c)\n",
    "    elif split =='val':\n",
    "        chairs_val.append(c)\n",
    "    else:\n",
    "        chairs_test.append(c)\n",
    "# print(len(chairs_train))\n",
    "# print(len(chairs_val))\n",
    "# print(len(chairs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_train = []\n",
    "tables_val = []\n",
    "tables_test = []\n",
    "for c in tables_meshes_paths: \n",
    "    try:\n",
    "        split = tables_split_dict[c.split(\"/\")[-1].replace(\".obj\", \"\")]\n",
    "    except KeyError:\n",
    "#         print(c.split(\"/\")[-1].replace(\".obj\", \"\"))\n",
    "        continue\n",
    "    if split =='train':\n",
    "        tables_train.append(c)\n",
    "    elif split =='val':\n",
    "        tables_val.append(c)\n",
    "    else:\n",
    "        tables_test.append(c)\n",
    "# print(len(tables_train))\n",
    "# print(len(tables_val))\n",
    "# print(len(tables_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_train.extend(tables_train)\n",
    "train_paths = chairs_train.copy()\n",
    "random.shuffle(train_paths)\n",
    "# train_paths = train_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_val.extend(tables_val)\n",
    "val_paths = chairs_val.copy()\n",
    "random.shuffle(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = len(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.346341600Z",
     "start_time": "2023-07-12T23:13:20.161258600Z"
    }
   },
   "outputs": [],
   "source": [
    "captions = pd.read_csv(\"captions_tablechair.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:20.449846300Z",
     "start_time": "2023-07-12T23:13:20.346341600Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for c in captions['description'].values:\n",
    "    cur = len(c.split(\" \"))\n",
    "    if cur>max_length:\n",
    "        max_length =cur\n",
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions=[]\n",
    "for index, row in captions.iterrows():\n",
    "    try:\n",
    "        if row[\"category\"]==\"Table\":\n",
    "            if tables_split_dict[row[\"modelId\"]]=='train':\n",
    "                train_captions.append(row['description'])\n",
    "        if row[\"category\"]==\"Chair\":\n",
    "            if chairs_split_dict[row[\"modelId\"]]=='train':\n",
    "                train_captions.append(row['description'])\n",
    "    except KeyError:\n",
    "        continue\n",
    "# print(len(train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.598342800Z",
     "start_time": "2023-07-12T23:13:20.449846300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.613926100Z",
     "start_time": "2023-07-12T23:13:21.598342800Z"
    }
   },
   "outputs": [],
   "source": [
    "def text2shape(paths, captions, tokenizer):\n",
    "    for path in paths:\n",
    "        # with open(path, 'rb') as obj_file:\n",
    "        mesh_dict = data_utils.load_process_mesh(path)\n",
    "#         mesh_dict['class_label'] = 18 if cls==\"Chair\" else 49\n",
    "#         if len(mesh_dict['vertices'])>200:\n",
    "#             continue\n",
    "        if len(mesh_dict['faces'])>2600:\n",
    "            continue\n",
    "        try:\n",
    "            text = captions[captions[\"modelId\"]==path.split(\"/\")[-1].replace(\".obj\", \"\")].sample(n=1)[\"description\"].values[0]\n",
    "        except:\n",
    "            continue\n",
    "        text = text.lower().replace(\"the\", '').replace(\"a\", '').replace(\"of\", '').replace(\"for\", '').replace(\"and\", '').replace(\"to\", '').replace(\"in\", '')\n",
    "        text = tokenizer.texts_to_sequences([text])[0]\n",
    "        mesh_dict['text_feature'] = np.pad(text, (0,max_length-len(text)))\n",
    "        yield mesh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.660455100Z",
     "start_time": "2023-07-12T23:13:21.613926100Z"
    }
   },
   "outputs": [],
   "source": [
    "Text2ShapeDataset = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(train_paths, captions, tk),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "#             'class_label': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "#             'class_label': tf.TensorShape(()),\n",
    "            'text_feature':tf.TensorShape(140)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:21.906000800Z",
     "start_time": "2023-07-12T23:13:21.706445600Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model_dataset = data_utils.make_vertex_model_dataset(Text2ShapeDataset, apply_random_shift=False)\n",
    "vertex_model_dataset = vertex_model_dataset.repeat()\n",
    "vertex_model_dataset = vertex_model_dataset.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset.output_shapes)\n",
    "vertex_model_dataset = vertex_model_dataset.prefetch(1)\n",
    "it = vertex_model_dataset.make_initializable_iterator()\n",
    "vertex_model_batch = it.get_next()\n",
    "iterator_init_op_train = it.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text2ShapeDatasetVal = tf.data.Dataset.from_generator(\n",
    "        lambda:text2shape(val_paths, captions, tk),\n",
    "        output_types={\n",
    "            'vertices': tf.int32, 'faces': tf.int32,\n",
    "#             'class_label': tf.int32,\n",
    "            'text_feature': tf.int32},\n",
    "        output_shapes={\n",
    "            'vertices': tf.TensorShape([None, 3]), 'faces': tf.TensorShape([None]),\n",
    "#             'class_label': tf.TensorShape(()),\n",
    "            'text_feature':tf.TensorShape(140)})\n",
    "vertex_model_dataset_val = data_utils.make_vertex_model_dataset(Text2ShapeDatasetVal, apply_random_shift=False)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.repeat()\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.padded_batch(BATCH_SIZE, padded_shapes=vertex_model_dataset_val.output_shapes)\n",
    "vertex_model_dataset_val = vertex_model_dataset_val.prefetch(1)\n",
    "itv = vertex_model_dataset_val.make_initializable_iterator()\n",
    "vertex_model_batch_val = itv.get_next()\n",
    "iterator_init_op_val = itv.initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/51/ala1s/deepmind-research/polygen/data_utils.py:162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(vertices), np.array(faces)\n"
     ]
    }
   ],
   "source": [
    "o = (i for i in text2shape(train_paths, captions, tk))\n",
    "TRAIN_SIZE = sum(1 for _ in o)\n",
    "o = (i for i in text2shape(val_paths, captions, tk))\n",
    "VAL_SIZE = sum(1 for _ in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5606"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:39.111517Z",
     "start_time": "2023-07-12T23:13:21.906000800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n",
      "Converted 8256 words (1212 misses)\n"
     ]
    }
   ],
   "source": [
    "vertex_model = modules.TextToVertexModel(\n",
    "    decoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=24,\n",
    "      dropout_rate=0.4,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True\n",
    "      ),\n",
    "    path_to_embeddings=\"glove.42B.300d.txt\",\n",
    "    embedding_dims = 300,\n",
    "    quantization_bits=8,\n",
    "    tokenizer=tk,\n",
    "    max_num_input_verts=5000,  # number of vertices in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "    use_discrete_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.650342600Z",
     "start_time": "2023-07-12T23:13:39.111517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vertices': <tf.Tensor 'IteratorGetNext:2' shape=(?, ?, 3) dtype=int32>, 'faces': <tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int32>, 'text_feature': <tf.Tensor 'IteratorGetNext:1' shape=(?, 140) dtype=int32>, 'vertices_flat': <tf.Tensor 'IteratorGetNext:3' shape=(?, ?) dtype=int32>, 'vertices_flat_mask': <tf.Tensor 'IteratorGetNext:4' shape=(?, ?) dtype=float32>}\n",
      "tfp.distributions.Categorical(\"vertex_model_2/vertex_model/create_dist/Categorical/\", batch_shape=[?, ?], event_shape=[], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vertex_model_pred_dist = vertex_model(vertex_model_batch)\n",
    "vertex_model_loss = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist.log_prob(vertex_model_batch['vertices_flat']) *\n",
    "    vertex_model_batch['vertices_flat_mask'])\n",
    "vertex_samples = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch, max_sample_length=1500, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)\n",
    "\n",
    "print(vertex_model_batch)\n",
    "print(vertex_model_pred_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model_pred_dist_val = vertex_model(vertex_model_batch_val)\n",
    "vertex_model_loss_val = -tf.reduce_sum(\n",
    "    vertex_model_pred_dist_val.log_prob(vertex_model_batch_val['vertices_flat']) *\n",
    "    vertex_model_batch_val['vertices_flat_mask'])\n",
    "vertex_samples_val = vertex_model.sample(\n",
    "    BATCH_SIZE, context=vertex_model_batch_val, max_sample_length=1500, top_p=0.95,\n",
    "    recenter_verts=False, only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"only_text_vars.pickle\", 'rb') as f:\n",
    "#     only_text_vars = pickle.load(f)\n",
    "\n",
    "# text_vars = []\n",
    "# for var in vertex_model.variables:\n",
    "#     if '/'.join((var._variable._name).split('/')[1:]) in only_text_vars:\n",
    "#         text_vars.append(var)\n",
    "# text_vars=tuple(text_vars)\n",
    "# # print(text_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.794703100Z",
     "start_time": "2023-07-12T23:13:57.779077200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pretrained_vars.pickle\", 'rb') as f:\n",
    "    common_vars = pickle.load(f)\n",
    "\n",
    "pretrained_vars = []\n",
    "for var in vertex_model.variables:\n",
    "    if '/'.join((var._variable._name).split('/')[1:]) in common_vars:\n",
    "        pretrained_vars.append(var)\n",
    "pretrained_vars=tuple(pretrained_vars)\n",
    "# print(pretrained_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:13:57.810357600Z",
     "start_time": "2023-07-12T23:13:57.794703100Z"
    }
   },
   "outputs": [],
   "source": [
    "vertex_model_saver = tf.train.Saver(var_list=pretrained_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:14:13.344725100Z",
     "start_time": "2023-07-12T23:13:57.825948300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create face model\n",
    "face_model = modules.FaceModel(\n",
    "      encoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=10,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "  decoder_config=dict(\n",
    "      hidden_size=512,\n",
    "      fc_size=2048,\n",
    "      num_heads=8,\n",
    "      layer_norm=True,\n",
    "      num_layers=14,\n",
    "      dropout_rate=0.2,\n",
    "      re_zero=True,\n",
    "      memory_efficient=True,\n",
    "      ),\n",
    "    class_conditional=False,\n",
    "    max_seq_length=8000, # number of faces in the input mesh, if this is lower than the number of vertices in the mesh, there will be errors during training\n",
    "    quantization_bits=8,\n",
    "    decoder_cross_attention=True,\n",
    "    use_discrete_vertex_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_samples_val = face_model.sample(\n",
    "    context=vertex_samples_val, max_sample_length=1000, top_p=0.95,\n",
    "    only_return_complete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T23:14:13.360755400Z",
     "start_time": "2023-07-12T23:14:13.344725100Z"
    }
   },
   "outputs": [],
   "source": [
    "face_model_saver = tf.train.Saver(var_list=face_model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 23:42:36.086412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-07-19 23:42:36.157777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.74\n",
      "pciBusID: 0000:65:00.0\n",
      "2023-07-19 23:42:36.157836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-19 23:42:36.190374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-07-19 23:42:36.330758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-19 23:42:36.332168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-19 23:42:36.379217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-07-19 23:42:36.421397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-07-19 23:42:36.421997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-07-19 23:42:36.422505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-07-19 23:42:36.453337: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 3899935000 Hz\n",
      "2023-07-19 23:42:36.454576: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x76d3800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-19 23:42:36.454617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-07-19 23:42:36.545555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x769afd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-19 23:42:36.545602: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-07-19 23:42:36.546154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 3090 major: 8 minor: 6 memoryClockRate(GHz): 1.74\n",
      "pciBusID: 0000:65:00.0\n",
      "2023-07-19 23:42:36.546215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-19 23:42:36.546272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-07-19 23:42:36.546297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-19 23:42:36.546320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-19 23:42:36.546344: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-07-19 23:42:36.546367: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-07-19 23:42:36.546389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-07-19 23:42:36.546872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-07-19 23:42:36.546925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-07-19 23:42:36.716865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-07-19 23:42:36.716893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-07-19 23:42:36.716897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-07-19 23:42:36.717275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22054 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/2803 [00:00<?, ?it/s]2023-07-19 23:43:37.156135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "/cluster/51/ala1s/deepmind-research/polygen/data_utils.py:162: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(vertices), np.array(faces)\n",
      "2023-07-19 23:43:37.938881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [21:28<00:00,  2.18it/s, loss=156.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 580.20966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [01:05<00:00,  6.91it/s, loss=685.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 545.78906\n",
      "- Found new best model, saving in text_gen_pretrained/best-1\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:40<00:00,  2.37it/s, loss=147.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 488.99664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=713.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 568.5297\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:39<00:00,  2.38it/s, loss=159.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 466.3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.63it/s, loss=713.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 600.6265\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:38<00:00,  2.38it/s, loss=135.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 389.41794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=843.733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 651.78143\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:40<00:00,  2.37it/s, loss=152.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 388.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=855.284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 705.25555\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:38<00:00,  2.38it/s, loss=151.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 338.52786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=910.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 746.0661\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:39<00:00,  2.38it/s, loss=135.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 312.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.63it/s, loss=1001.966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 832.432\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:39<00:00,  2.38it/s, loss=131.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 274.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.63it/s, loss=1093.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 890.1261\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:41<00:00,  2.37it/s, loss=104.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 238.92009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=1115.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 949.01544\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2803/2803 [19:43<00:00,  2.37it/s, loss=96.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train loss: 217.06757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 451/451 [00:59<00:00,  7.61it/s, loss=1216.680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics: 982.214\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████▍                                                      | 1084/2803 [07:36<13:04,  2.19it/s, loss=142.023]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "last_saver = tf.train.Saver(var_list=vertex_model.variables) # will keep last 5 epochs\n",
    "best_saver = tf.train.Saver(var_list=vertex_model.variables, max_to_keep=2)  # only keep 1 best checkpoint (best on eval)\n",
    "\n",
    "# %matplotlib inline \n",
    "learning_rate = 5e-4\n",
    "training_steps = 500\n",
    "check_step_metrics = 10\n",
    "check_step_samples = 100\n",
    "EPOCHS = 25\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# vertex_model_optim_op = optimizer.minimize(vertex_model_loss, var_list=text_vars)\n",
    "vertex_model_optim_op = optimizer.minimize(vertex_model_loss, var_list=vertex_model.variables)\n",
    "best_v_loss = float('inf')\n",
    "# Training loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    vertex_model_saver.restore(sess, \"vertex_model/model\")\n",
    "    face_model_saver.restore(sess, 'face_model/model')\n",
    "    \n",
    "#     train_writer = s.FileWriter(os.path.join(log_dir, 'train_summaries'), sess.graph)\n",
    "#     eval_writer = s.FileWriter(os.path.join(log_dir, 'eval_summaries'), sess.graph)\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "        print(\"Epoch {}/{}\".format(e + 1, EPOCHS))\n",
    "        num_steps = (TRAIN_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_train)\n",
    "        t = trange(num_steps)\n",
    "        loss_values = []\n",
    "        for i in t:\n",
    "            sess.run(vertex_model_optim_op)\n",
    "\n",
    "            loss_val = sess.run(vertex_model_loss)\n",
    "            loss_values.append(loss_val)\n",
    "            # Log the loss in the tqdm progress bar\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        print(\"- Train loss: \" + str(mean_loss))\n",
    "        \n",
    "        last_save_path = os.path.join(\"text_gen_pretrained\", 'last')\n",
    "        last_saver.save(sess, last_save_path, global_step=e + 1)\n",
    "    \n",
    "        num_steps = (VAL_SIZE + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        sess.run(iterator_init_op_val)\n",
    "        loss_values = []\n",
    "        t = trange(num_steps)\n",
    "        for i in t:\n",
    "            loss_val = sess.run(vertex_model_loss_val)\n",
    "            loss_values.append(loss_val)\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        mean_loss = np.array(loss_values).mean()\n",
    "        print(\"- Eval metrics: \" + str(mean_loss))\n",
    "\n",
    "        if mean_loss<=best_v_loss:\n",
    "            best_v_loss = mean_loss\n",
    "            best_save_path = os.path.join(\"text_gen_pretrained\", 'best')\n",
    "            best_save_path = best_saver.save(sess, best_save_path, global_step=e + 1)\n",
    "            print(\"- Found new best model, saving in {}\".format(best_save_path))\n",
    "                \n",
    "        #SAmples\n",
    "        if e>20:\n",
    "            sess.run(iterator_init_op_val)\n",
    "            v_samples_np, f_samples_np, b_np = sess.run((vertex_samples_val, face_samples_val, vertex_model_batch_val))\n",
    "            mesh_list = []\n",
    "            for n in range(BATCH_SIZE):\n",
    "                mesh_list.append({\n",
    "                    'vertices': v_samples_np['vertices'][n][:v_samples_np['num_vertices'][n]],\n",
    "                    'faces': data_utils.unflatten_faces(\n",
    "                        f_samples_np['faces'][n][:f_samples_np['num_face_indices'][n]])\n",
    "                    })\n",
    "            data_utils.plot_meshes(mesh_list, ax_lims=0.5)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last1",
   "language": "python",
   "name": "last1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
